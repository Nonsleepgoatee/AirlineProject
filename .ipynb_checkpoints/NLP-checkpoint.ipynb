{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-03T20:28:40.909645Z",
     "start_time": "2024-05-03T20:28:39.872602Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/chungkaichou/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chungkaichou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/chungkaichou/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T20:28:41.195976Z",
     "start_time": "2024-05-03T20:28:40.910388Z"
    }
   },
   "id": "17f729a9f735d8bb",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df=pd.read_csv('cleaned_data.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T20:28:43.436720Z",
     "start_time": "2024-05-03T20:28:42.171313Z"
    }
   },
   "id": "e5e4ab3a166faee6",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def text_preprocessor(text):\n",
    "    text=re.sub(r'http[s]?://\\S+', '', text)\n",
    "    text=re.sub(r'@\\w+','',text)\n",
    "    text=re.sub(r'#','',text)\n",
    "    text=re.sub(r'\\d+','',text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens=[word.lower() for word in tokens]\n",
    "    tokens=[word for word in tokens if word not in string.punctuation]\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filted_tokens=[word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    lemmatized_tokens=[lemmatizer.lemmatize(token) for token in filted_tokens]\n",
    "    preprocessed_text=' '.join(lemmatized_tokens)\n",
    "    return preprocessed_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T15:47:09.014104Z",
     "start_time": "2024-05-03T15:47:09.011652Z"
    }
   },
   "id": "5b47624b07d00165",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['preprocessed_text']=df['review'].apply(text_preprocessor)\n",
    "X=df['preprocessed_text']\n",
    "y=df['Recommended']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)\n",
    "combined_vectorizer=FeatureUnion([('tfidf',TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode', analyzer='word', ngram_range=(1,3), use_idf=True, sublinear_tf=True, smooth_idf=True, stop_words='english')),('CountVectorizer',CountVectorizer(min_df=3, max_features=None, strip_accents='unicode', analyzer='word', ngram_range=(1,3), stop_words='english'))])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T15:48:58.906827Z",
     "start_time": "2024-05-03T15:47:09.019035Z"
    }
   },
   "id": "3b9415d481d96ee9",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "            Classifier       Vectorizer  Accuracy\n0          Naive Bayes            tfidf  0.885452\n1  Logistic Regression            tfidf  0.921954\n2        Random Forest            tfidf  0.893033\n3          Naive Bayes  CountVectorizer  0.869911\n4  Logistic Regression  CountVectorizer  0.923357\n5        Random Forest  CountVectorizer  0.893223\n6          Naive Bayes         Combined  0.873323\n7  Logistic Regression         Combined  0.923357\n8        Random Forest         Combined  0.893336",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Classifier</th>\n      <th>Vectorizer</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Naive Bayes</td>\n      <td>tfidf</td>\n      <td>0.885452</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Logistic Regression</td>\n      <td>tfidf</td>\n      <td>0.921954</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Random Forest</td>\n      <td>tfidf</td>\n      <td>0.893033</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Naive Bayes</td>\n      <td>CountVectorizer</td>\n      <td>0.869911</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Logistic Regression</td>\n      <td>CountVectorizer</td>\n      <td>0.923357</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Random Forest</td>\n      <td>CountVectorizer</td>\n      <td>0.893223</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Naive Bayes</td>\n      <td>Combined</td>\n      <td>0.873323</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Logistic Regression</td>\n      <td>Combined</td>\n      <td>0.923357</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Random Forest</td>\n      <td>Combined</td>\n      <td>0.893336</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier={'Naive Bayes':MultinomialNB(),\n",
    "            'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "            'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "vectorizer = {\n",
    "    'tfidf': TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode', analyzer='word', ngram_range=(1,3), use_idf=True, sublinear_tf=True, smooth_idf=True, stop_words='english'),\n",
    "    'CountVectorizer': CountVectorizer(min_df=3, max_features=None, strip_accents='unicode', analyzer='word', ngram_range=(1,3), stop_words='english'),\n",
    "    'Combined': combined_vectorizer\n",
    "}\n",
    "\n",
    "MLresult=[]\n",
    "for vec_name, vec in vectorizer.items():\n",
    "    for clf_name,clf in classifier.items():\n",
    "        pipe=Pipeline([\n",
    "            (vec_name,vec),\n",
    "            (clf_name,clf)\n",
    "        ])\n",
    "        pipe.fit(X_train,y_train)\n",
    "        y_pred=pipe.predict(X_test)\n",
    "        accuracy=accuracy_score(y_test,y_pred)\n",
    "        MLresult.append({\n",
    "            'Classifier': clf_name,\n",
    "            'Vectorizer': vec_name,\n",
    "            'Accuracy':accuracy\n",
    "        })\n",
    "MLresult=pd.DataFrame(MLresult)\n",
    "MLresult"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T16:24:56.273657Z",
     "start_time": "2024-05-03T15:48:58.907918Z"
    }
   },
   "id": "cb4f3359b3557d4b",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#advanced NLP\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "import tokenization"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-03T20:41:36.563570Z"
    }
   },
   "id": "5fe383555523a62c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ed208e886865b148"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
